{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/04_classification/08_video_processing_project/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khlO4Bu21oZ4"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlzIlBsScJJ_"
      },
      "source": [
        "# Video Classification with Pre-Trained Models Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTirVS4FWaPx"
      },
      "source": [
        "In this project we will import a pre-existing model that recognizes objects and use the model to identify those objects in a video. We'll edit the video to draw boxes around the identified object, and then we'll reassemble the video so the boxes are shown around objects in the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTVUYxPwcHhp"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIOgOHP1ces"
      },
      "source": [
        "## Exercise 1: Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTEOK1ZmqN8"
      },
      "source": [
        "You will process a video frame by frame, identify objects in each frame, and draw a bounding box with a label around each car in the video.\n",
        " \n",
        "Use the [SSD MobileNet V1 Coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) (*ssd_mobilenet_v1_coco*) model. The video you'll process can be found [on Pixabay](https://pixabay.com/videos/cars-motorway-speed-motion-traffic-1900/). The 640x360 version of the video is smallest and easiest to handle, though any size should work since you must scale down the images for processing.\n",
        " \n",
        "Your program should:\n",
        " \n",
        "* ~~Read in a video file (use the one in this colab if you want)~~\n",
        "* ~~Load the TensorFlow model linked above~~\n",
        "* ~~Loop over each frame of the video~~\n",
        "* ~~Scale the frame down to a size the model expects~~\n",
        "* ~~Feed the frame to the model~~\n",
        "* ~~Loop over detections made by the model~~\n",
        "* ~~If the detection score is above some threshold, draw a bounding box onto the frame and put a label in or near the box~~\n",
        "* ~~Write the frame back to a new video~~\n",
        " \n",
        "Some tips:\n",
        " \n",
        "* Processing an entire video is slow, so consider truncating the video or skipping over frames during development. Skipping frames will make the video choppy. But you'll be able to see a wider variety of images than you would with a truncated video with all of the original frames in the clip.\n",
        "* The model expects a 300x300 image. You'll likely have to scale your frames to fit the model. When you get a bounding box, that box is relative to the scaled image. You'll need to scale the bounding box out to the original image size.\n",
        "* Don't start by trying to process the video. Instead, capture one frame and work with it until you are happy with your object detection, bounding boxes, and labels. Once you get those done, use the same logic on the other frames of the video.\n",
        "* The [Coco labels file](https://github.com/nightrome/cocostuff/blob/master/labels.txt) can be used to identify classified objects.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XM35vYWSbim"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports "
      ],
      "metadata": {
        "id": "9ylRpj3sxOsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import os\n",
        "import tarfile\n",
        "import shutil\n"
      ],
      "metadata": {
        "id": "O4ak1zcLxUo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the TensorFlow Model "
      ],
      "metadata": {
        "id": "C8aDXyhSxisW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaing the model file\n",
        "url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "\n",
        "filename = 'ssd_mobilenet_v1_coco_2018_01_28.tar.gz'\n",
        "\n",
        "url1 = url + filename\n",
        "\n",
        "urllib.request.urlretrieve(url1, filename)\n",
        "\n",
        "dir_name= filename[0:-len('.tar.gz')]"
      ],
      "metadata": {
        "id": "OZzs9SAvxuTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting model data\n",
        "dir_name = filename[0:-len('.tar.gz')]\n",
        "\n",
        "if os.path.exists(dir_name):\n",
        "  shutil.rmtree(dir_name) \n",
        "\n",
        "tarfile.open(filename, 'r:gz').extractall('./')\n",
        "\n",
        "os.listdir(dir_name)"
      ],
      "metadata": {
        "id": "e_tHXU_qx3Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph"
      ],
      "metadata": {
        "id": "iGud56EmBbtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining frozen graph\n",
        "frozen_graph = os.path.join(dir_name, 'frozen_inference_graph.pb')\n",
        "\n",
        "with tf.io.gfile.GFile(frozen_graph, \"rb\") as f:\n",
        "  graph_def = tf.compat.v1.GraphDef()\n",
        "  loaded = graph_def.ParseFromString(f.read())"
      ],
      "metadata": {
        "id": "5KcjdW5w8it3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wrapping Graph\n",
        "def wrap_graph(graph_def, inputs, outputs, print_graph=False):\n",
        "  wrapped = tf.compat.v1.wrap_function(\n",
        "    lambda: tf.compat.v1.import_graph_def(graph_def, name=\"\"), [])\n",
        "\n",
        "  return wrapped.prune(\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, inputs),\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, outputs))\n",
        "    \n"
      ],
      "metadata": {
        "id": "4VKHEG8h9OLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#From Git Hib\n",
        "dict = {\n",
        "    0:\"background\",\n",
        "    1:\"person\",\n",
        "    2:\"bicycle\",\n",
        "    3:\"car\",\n",
        "    4:\"motorcycle\",\n",
        "    5:\"airplane\",\n",
        "    6:\"bus\",\n",
        "    7:\"train\",\n",
        "    8:\"truck\",\n",
        "    9:\"boat\",\n",
        "    10:\"trafficlight\",\n",
        "    11:\"firehydrant\",\n",
        "    12:\"unknown\",\n",
        "    13:\"stopsign\",\n",
        "    14:\"parkingmeter\",\n",
        "    15:\"bench\",\n",
        "    16:\"bird\",\n",
        "    17:\"cat\",\n",
        "    18:\"dog\",\n",
        "    19:\"horse\",\n",
        "    20:\"sheep\",\n",
        "    21:\"cow\",\n",
        "    22:\"elephant\",\n",
        "    23:\"bear\",\n",
        "    24:\"zebra\",\n",
        "    25:\"giraffe\",\n",
        "    26:\"unknown\",\n",
        "    27:\"backpack\",\n",
        "    28:\"umbrella\",\n",
        "    29:\"unknown\",\n",
        "    30:\"unknown\",\n",
        "    31:\"handbag\",\n",
        "    32:\"tie\",\n",
        "    33:\"suitcase\",\n",
        "    34:\"frisbee\",\n",
        "    35:\"skis\",\n",
        "    36:\"snowboard\",\n",
        "    37:\"sportsball\",\n",
        "    38:\"kite\",\n",
        "    39:\"baseballbat\",\n",
        "    40:\"baseballglove\",\n",
        "    41:\"skateboard\",\n",
        "    42:\"surfboard\",\n",
        "    43:\"tennisracket\",\n",
        "    44:\"bottle\",\n",
        "    45:\"unknown\",\n",
        "    46:\"wineglass\",\n",
        "    47:\"cup\",\n",
        "    48:\"fork\",\n",
        "    49:\"knife\",\n",
        "    50:\"spoon\",\n",
        "    51:\"bowl\",\n",
        "    52:\"banana\",\n",
        "    53:\"apple\",\n",
        "    54:\"sandwich\",\n",
        "    55:\"orange\",\n",
        "    56:\"broccoli\",\n",
        "    57:\"carrot\",\n",
        "    58:\"hotdog\",\n",
        "    59:\"pizza\",\n",
        "    60:\"donut\",\n",
        "    61:\"cake\",\n",
        "    62:\"chair\",\n",
        "    63:\"couch\",\n",
        "    64:\"pottedplant\",\n",
        "    65:\"bed\",\n",
        "    66:\"unknown\",\n",
        "    67:\"diningtable\",\n",
        "    68:\"unknown\",\n",
        "    69:\"unknown\",\n",
        "    70:\"toilet\",\n",
        "    71:\"unknown\",\n",
        "    72:\"tv\",\n",
        "    73:\"laptop\",\n",
        "    74:\"mouse\",\n",
        "    75:\"remote\",\n",
        "    76:\"keyboard\",\n",
        "    77:\"cellphone\",\n",
        "    78:\"microwave\",\n",
        "    79:\"oven\",\n",
        "    80:\"toaster\",\n",
        "    81:\"sink\",\n",
        "    82:\"refrigerator\",\n",
        "    83:\"unknown\",\n",
        "    84:\"book\",\n",
        "    85:\"clock\",\n",
        "    86:\"vase\",\n",
        "    87:\"scissors\",\n",
        "    88:\"teddybear\",\n",
        "    89:\"hairdrier\",\n",
        "    90:\"toothbrush\"\n",
        "}"
      ],
      "metadata": {
        "id": "8mTUMSsI9gcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "def drawBoxes(frame):\n",
        "  image = frame\n",
        "\n",
        "  outputs = (\n",
        "    'num_detections:0',\n",
        "    'detection_classes:0',\n",
        "    'detection_scores:0',\n",
        "    'detection_boxes:0',\n",
        "  )\n",
        "  input_images = [image]\n",
        "      \n",
        "  model = wrap_graph(graph_def=graph_def,\n",
        "                    inputs=[\"image_tensor:0\"],\n",
        "                    outputs=outputs)\n",
        "\n",
        "  tensor = tf.convert_to_tensor(input_images, dtype=tf.uint8)\n",
        "\n",
        "  detections = model(tensor)\n",
        "  boxes = []\n",
        "  i = 0\n",
        "  while detections[3][0][i].numpy().any():\n",
        "    boxes.append((detections[3][0][i].numpy(), detections[1][0][i].numpy()))\n",
        "    i += 1\n",
        "\n",
        "  height = image.shape[0]\n",
        "  width = image.shape[1]\n",
        "  for box in boxes:\n",
        "    label = box[1]\n",
        "    y1 = int(box[0][0] * height)\n",
        "    x1 = int(box[0][1] * width)\n",
        "    y2 = int(box[0][2] * height)  \n",
        "    x2 = int(box[0][3] * width)\n",
        "    image = cv.rectangle(image, (x1, y1), (x2, y2), (0,255,0), 2)\n",
        "    cv.putText(image, dict[label], (x1,y2), cv.FONT_HERSHEY_SIMPLEX, .8, [0,0,0], 2)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "v01rin9L91dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video"
      ],
      "metadata": {
        "id": "75UI3yGzy0im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Video"
      ],
      "metadata": {
        "id": "ctQOz64ZRvFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading file\n",
        "# Takes a minute to run\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "ooR1ONwOy5_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Video and Model"
      ],
      "metadata": {
        "id": "LFhBzWoMR4Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The video is stored here\n",
        "video = cv.VideoCapture('Cars.mp4')\n"
      ],
      "metadata": {
        "id": "L1gFwXU7AIvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Information about the video\n",
        "height = int(video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "fps = video.get(cv.CAP_PROP_FPS)\n",
        "total_frames = int(video.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "print(f'height: {height}')\n",
        "print(f'width: {width}')\n",
        "print(f'frames per second: {fps}')\n",
        "print(f'total frames: {total_frames}')\n",
        "print(f'video length (seconds): {total_frames / fps}')"
      ],
      "metadata": {
        "id": "P-TCCasUAK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code block defines the output video\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "output_video = cv.VideoWriter('Updated Cars Video.mp4', fourcc, fps, (width, height))"
      ],
      "metadata": {
        "id": "kHj6V1OYAbZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This prints each frame, dectects the objects, and places frame in New Cars Video\n",
        "#This code takes 5-7 minutes to run \n",
        "for i in range(0, total_frames, 25):\n",
        "  print(i, \" / \", total_frames)\n",
        "  video.set(cv.CAP_PROP_POS_FRAMES, i)\n",
        "  ret, frame = video.read()\n",
        "  frame = drawBoxes(frame)\n",
        "  if not ret:\n",
        "    raise Exception(\"Problem reading frame\", i, \" from video\")\n",
        "  output_video.write(frame)"
      ],
      "metadata": {
        "id": "dpYQa2_TAfby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Releases original video\n",
        "video.release()"
      ],
      "metadata": {
        "id": "-HwAqkbOBGiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Releases Updated video\n",
        "output_video.release()"
      ],
      "metadata": {
        "id": "4luocZ47BHmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Ethical Implications"
      ],
      "metadata": {
        "id": "4XLEiIvC_iyv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4FvC1Aa0ZT5"
      },
      "source": [
        "Even the most basic models have the potential to affect segments of the population in different ways. It is important to consider how your model might positively and negatively affect different types of users.\n",
        "\n",
        "In this section of the project, you will reflect on the positive and negative implications of your model. Frame the context of your model creation using this narrative:\n",
        "\n",
        "> The city of Seattle is attempting to reduce traffic congestion in its downtown area. As part of this project, they plan to allow each local driver one free trip to downtown Seattle per week. After that, the driver will have to pay a $50 toll for each extra day per week driven. As an early proof of concept for this project, your team is tasked with using machine learning to correctly identify automobiles on the road. The next phase of the project will involve detecting license plate numbers and then cross-referencing that data with RFID chips that should be mounted in all local drivers' cars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkyzwVQr0brd"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy4I2vG60ebd"
      },
      "source": [
        "**Positive Impact**\n",
        "\n",
        "Your model is trying to solve a problem. Think about who will benefit from that problem being solved and write a brief narrative about how the model will help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59MK1Ah0fWy"
      },
      "source": [
        "> *The city of Seattle will benefit because the problem stated is that there is a lot of traffic in downtown Seattle which could lead to less car accidents and therefore less car related deaths. The city of Seattle also benifits because the earned money from the $50 tolls everyday. This model will help identify the license plates of the car driving so the city will know who to bill to. Another group who can benefit are people walking downtown due to lack of traffic.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzqkrLnk0hMU"
      },
      "source": [
        "**Negative Impact**\n",
        "\n",
        "Models rarely benefit everyone equally. Think about who might be negatively impacted by the predictions your model is making. This person(s) might not be directly using the model, but they might be impacted indirectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hefa1JdP0kj3"
      },
      "source": [
        "> *Businesses in downtown Seattle as well as workers will be negatively impacted. Businesses in downtown Seattle will be negatively impacted because people who would have drove downtown just for the business may not because using the toll becomes expensive. People who work downtown will also be negatively impacted because they would have spend more money simply driving to work.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uax2HAzd0mHX"
      },
      "source": [
        "**Bias**\n",
        "\n",
        "Models can be biased for many reasons. The bias can come from the data used to build the model (e.g., sampling, data collection methods, available sources) and/or from the interpretation of the predictions generated by the model.\n",
        "\n",
        "Think of at least two ways bias might have been introduced to your model and explain both below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bJGm-qs0oQV"
      },
      "source": [
        "> *One source of bias in the model could be that the model may not be able to detect the license plates in other weather besides being sunny. For example if it were raining I questioned if the model could detect the license plate.*\n",
        "\n",
        "> *Another source of bias in the model could be license plates that are in the windows of the car. For instance in a way to avoid paying the fines drivers could put the license plates in the window. In that case police wouldnt pull them over and the camerea may not be able to dectact the plate.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybb1zAkC0p2e"
      },
      "source": [
        "**Changing the Dataset to Mitigate Bias**\n",
        "\n",
        "Having bias in your dataset is one of the primary ways in which bias is introduced to a machine learning model. Look back at the input data you fed to your model. Think about how you might change something about the data to reduce bias in your model.\n",
        "\n",
        "What change or changes could you make to reduce the bias in your dataset? Consider the data you have, how and where it was collected, and what other sources of data might be used to reduce bias.\n",
        "\n",
        "Write a summary of changes that could be made to your input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFsnF4_h08DD"
      },
      "source": [
        "> *Since the data has potential with the weather and time of day the original video was taken we can increase the amount of cars in the dataset so that the model can take account of the weather and different times of day.* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChEJbhXA02pW"
      },
      "source": [
        "**Changing the Model to Mitigate Bias**\n",
        "\n",
        "Is there any way to reduce bias by changing the model itself? This could include modifying algorithmic choices, tweaking hyperparameters, etc.\n",
        "\n",
        "Write a brief summary of changes you could make to help reduce bias in your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEAhgO_U0p8Y"
      },
      "source": [
        "> *Since the model has potential bias in the weather conditions when recording data and making prediction, we can adjust the data by including recording of cars in all kinds of weather conditions. We wouldnt suggest changing the model itself yet because this bias could be fixed with more data.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rShB5BQv0wix"
      },
      "source": [
        "**Mitigating Bias Downstream**\n",
        "\n",
        "Models make predictions. Downstream processes make decisions. What processes and/or rules should be in place for people and systems interpreting and acting on the results of your model to reduce bias? Describe these rules and/or processes below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C__BwBP-00HN"
      },
      "source": [
        "> *Since the predictions have potential bias in including all the circumstances a rule that could be put in place for people and systems interpreting and acting on the results of the model is too simply take into account that the license plates maybe hard to read in certin weather conditions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L_4RNXphYtI"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "C8aDXyhSxisW",
        "iGud56EmBbtK"
      ],
      "name": "colab.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.16"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
